{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a95931-510e-48df-8383-8cc0c26ad208",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7168460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 38148451\n",
      "First 100 characters of the dataset:\n",
      "from rest_framework_gis import serializers\n",
      "from rest_framework import serializers as s\n",
      "\n",
      "from .models import (\n",
      "    Artificialisee2015to2018,\n",
      "    Artificielle2018,\n",
      "    CommunesSybarval,\n",
      "    CouvertureSol,\n",
      "    EnveloppeUrbaine2018,\n",
      "    Ocsge,\n",
      "    Renaturee2018to2015,\n",
      "    Sybarval,\n",
      "    Voirie2018,\n",
      "    ZonesBaties2018,\n",
      "    UsageSol,\n",
      ")\n",
      "\n",
      "\n",
      "def get_label(code=\"\", label=\"\"):\n",
      "    if code is None:\n",
      "        code = \"-\"\n",
      "    if label is None:\n",
      "        label = \"inconnu\"\n",
      "    return f\"{code} {label[:30]}\"\n",
      "\n",
      "\n",
      "class Artificialisee2015to2018Serializer(serializers.GeoFeatureModelSerializer):\n",
      "    usage_2015 = s.SerializerMethodField()\n",
      "    usage_2018 = s.SerializerMethodField()\n",
      "    couverture_2015 = s.SerializerMethodField()\n",
      "    couverture_2018 = s.SerializerMethodField()\n",
      "\n",
      "    def get_usage_2015(self, obj):\n",
      "        return get_label(code=obj.us_2015, label=obj.us_2015_label)\n",
      "\n",
      "    def get_usage_2018(self, obj):\n",
      "        return get_label(code=obj.us_2018, label=obj.us_2018_label)\n",
      "\n",
      "    def get_couverture_2015(self, ob\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the dataset:\", len(text))\n",
    "print(\"First 100 characters of the dataset:\")\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2953e2-a476-400b-ae51-b3a6ed539089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the unique characters:\t\n",
      "\f !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â Â¡Â£Â¨Â©ÂªÂ«Â°Â±Â²Â´ÂµÂ·ÂºÂ»Ã€ÃÃƒÃ„Ã…Ã‡Ã‰Ã‹ÃÃÃ“Ã•Ã–Ã—Ã˜ÃšÃœÃÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã¸Ã¹ÃºÃ¼Ã½Ã¾ÄÄƒÄ…Ä‡Ä‰ÄŒÄÄÄ‘Ä“Ä—Ä™Ä›ÄÄŸÄ©Ä«Ä¯Ä°Ä±Ä¾ÅÅ‚Å„ÅˆÅ‹Å‘Å“Å˜Å™Å›ÅÅŸÅ Å¡Å£Å¥Å©Å«Å±Å³Å¹ÅºÅ¼Å¾ÆÆ¡Æ°ÇÇÇ”È™È›É‘É’É”É™É›ÉœÉ£É¤ÉªÉ¯É²Ê‚ÊƒÊˆÊŠÊŒÊÊ’Ê”Ê¤Ê§Ê°Ê³Ê·ËˆËŒËœÌƒÌ†ÌˆÌŠÌ Í¡Î†ÎˆÎ‘Î’Î“Î”Î•Î›ÎœÎŸÎ Î£Î¤Î¥Î¦Î©Î¬Î­Î®Î¯Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏ‚ÏƒÏ„Ï…Ï†Ï‡Ï‰ÏŒÏÏĞ†ĞĞ‘Ğ’Ğ“Ğ”Ğ•Ğ—Ğ˜Ğ™ĞšĞ›ĞœĞĞĞŸĞ Ğ¡Ğ¢Ğ£Ğ¤Ğ¦Ğ§Ğ«Ğ­Ğ¯Ğ°Ğ±Ğ²Ğ³Ğ´ĞµĞ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑÑÑ‘Ñ”Ñ–Ñ—ÑÔ±Ô¾Õ„Õ†ÕÕ¡Õ¢Õ£Õ¥Õ§Õ¨Õ©Õ«Õ¬Õ­Õ®Õ¯Õ°Õ²Õ³Õ´ÕµÕ¶Õ·Õ¸Õ¹ÕºÕ»Õ½Õ¾Õ¿Ö€ÖÖ‚Ö¸Ö¹Ö¼×××‘×’×“×”×•×–×—×˜×™×›×œ×××Ÿ× ×¡×¢×¤×¥×¦×§×¨×©×ªØ¡Ø¢Ø£Ø¤Ø¥Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙ‰ÙŠÙ”Ù¤Ù¹Ù¾Ú†ÚˆÚ‘Ú˜Ú©Ú¯ÚºÚ¾ÛÛŒÛ’à¤à¤‚à¤…à¤†à¤‡à¤‰à¤à¤“à¤”à¤•à¤–à¤—à¤šà¤œà¤Ÿà¤ à¤¡à¤£à¤¤à¤¥à¤¦à¤§à¤¨à¤ªà¤«à¤¬à¤­à¤®à¤¯à¤°à¤²à¤µà¤¶à¤·à¤¸à¤¹à¤¼à¤¾à¤¿à¥€à¥à¥‚à¥‡à¥ˆà¥‹à¥Œà¥à¦à¦‚à¦ƒà¦…à¦†à¦‡à¦ˆà¦‰à¦Šà¦‹à¦à¦à¦“à¦”à¦•à¦–à¦—à¦˜à¦™à¦šà¦›à¦œà¦à¦à¦Ÿà¦ à¦¡à¦¢à¦£à¦¤à¦¥à¦¦à¦§à¦¨à¦ªà¦«à¦¬à¦­à¦®à¦¯à¦°à¦²à¦¶à¦·à¦¸à¦¹à¦¼à¦¾à¦¿à§€à§à§‚à§ƒà§‡à§ˆà§‹à§Œà§à§à§—à§œà§à§Ÿà§¦à§§à§¨à§©à§ªà§«à§¬à§­à§®à§¯à§°à®…à®†à®‡à®à®’à®“à®•à®™à®šà®à®Ÿà®£à®¤à®¨à®©à®ªà®®à®¯à®°à®±à®²à®³à®´à®µà®¾à®¿à¯€à¯à¯‚à¯†à¯‡à¯ˆà¯Šà¯‹à¯à°‚à°…à°†à°‡à°‰à°“à°”à°•à°–à°—à°˜à°šà°œà°Ÿà°¡à°¤à°¦à°§à°¨à°ªà°«à°¬à°®à°¯à°°à°²à°µà°¶à°·à°¸à°¹à°¾à°¿à±€à±à±‚à±ƒà±†à±‡à±ˆà±Šà±‹à±à±–à¸à¸‚à¸„à¸‡à¸‰à¸”à¸•à¸™à¸šà¸¡à¸¢à¸§à¸ªà¸­à¸±à¸²à¸³à¸´à¸µà¹„à¹ˆà¹‰áƒáƒ‘áƒ’áƒ“áƒ”áƒ•áƒ–áƒ—áƒ˜áƒ™áƒšáƒ›áƒœáƒáƒáƒ áƒ¡áƒ¢áƒ£áƒ¤áƒ¥áƒ¦áƒ§áƒ¨áƒ©áƒªáƒ«áƒ¬áƒ­áƒ®áš áš»á›‡áµ€áµ‰áº¡áº£áº¥áº§áº©áº«áº­áº¯áº±áº³áºµáº·áº¹áº»áº½áº¿á»á»ƒá»…á»‡á»‰á»‹á»á»á»‘á»“á»•á»—á»™á»›á»á»á»Ÿá»¡á»£á»¥á»§á»©á»«á»­á»¯á»±á»³á»µá»·á»¹â€‹â€Œâ€â€“â€”â€˜â€™â€œâ€â€â€¢â€¦â€°â€²â€ºâ€¼â´â‚–â‚¬â„ƒâ„“â†â†‘â†’â†“â†©â‡’â‡¦â‡§â‡¨â‡©âˆ€âˆ‚âˆƒâˆˆâˆ’âˆšâˆ§âˆ¨âˆ«â‰ â‰¤â‰¥âŠƒâŠ›â‹…âŒšâ¢â”€â”â”‚â”â”’â””â”˜â”šâ”œâ”²â”µâ”»â•â•‘â•”â•—â•šâ•â• â•¯â–ˆâ–‰â–Œâ–‘â–¡â–¶â—â˜â˜â™¥âš–âš™âš âš¡âœ…âœ‰âœ“âœ”âœ˜âœ¨âœªâœ¿âŒââ“â”â¯â–âœâ¡âŸ¢âŸ©âŸ«âŸ³â¬…â¬†â¬‡ã€€ã€ã€‚ã€‹ã€Œã€ã€ã€‘ã€˜ã€™ã€œã‚ã„ã†ãˆãŠã‹ãŒããããã‘ã’ã“ã”ã•ã—ã˜ã™ãšã›ãããŸã ã¡ã£ã¤ã¦ã§ã¨ã©ãªã«ã­ã®ã¯ã°ã³ã¸ã¹ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚ƒã‚„ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“ã‚¡ã‚¢ã‚£ã‚¤ã‚¦ã‚§ã‚¨ã‚©ã‚ªã‚«ã‚¬ã‚­ã‚®ã‚¯ã‚°ã‚±ã‚²ã‚³ã‚´ã‚µã‚¶ã‚·ã‚¸ã‚¹ã‚ºã‚»ã‚½ã‚¿ãƒ€ãƒãƒƒãƒ†ãƒ‡ãƒˆãƒ‰ãƒŠãƒ‹ãƒãƒãƒãƒ‘ãƒ“ãƒ”ãƒ•ãƒ–ãƒ—ãƒ™ãƒšãƒœãƒãƒãƒŸãƒ ãƒ¡ãƒ¢ãƒ£ãƒ¥ãƒ¦ãƒ§ãƒ©ãƒªãƒ«ãƒ¬ãƒ­ãƒ¯ãƒ³ãƒ»ãƒ¼ã„´ã„¹ä¸€ä¸‚ä¸ƒä¸ˆä¸‰ä¸Šä¸‹ä¸ä¸ä¸”ä¸–ä¸šä¸œä¸¤ä¸¥ä¸¦ä¸ªä¸­ä¸°ä¸²ä¸¸ä¸ºä¸»ä¸¾ä¹…ä¹ˆä¹‰ä¹‹ä¹ä¹˜ä¹Ÿä¹ ä¹¦ä¹°ä¹±ä¹³äº†äºˆäº‰äº‹äºŒäºäº‘äº’äº”äºšäº›äº¤äº§äº«äº­äº®äººä»€ä»…ä»Šä»ä»“ä»–ä»˜ä»£ä»¤ä»¥ä»¬ä»®ä»¶ä»·ä»»ä»½ä»¿ä¼‘ä¼—ä¼˜ä¼šä¼ ä¼ªä¼°ä¼¼ä½†ä½ˆä½ä½ä½“ä½”ä½•ä½™ä½œä½ ä½¿ä¾†ä¾‹ä¾›ä¾ä¾§ä¾¯ä¾µä¾¿ä¿ä¿¡ä¿®å€‹å€å€’å€™å€šå€Ÿå€¤å€¼å‡ååšåœå¥å´åµå¶å‚…å‚å‚™å‚¨åƒåƒ¹å„²å…ƒå…„å……å…ˆå…å…¥å…§å…¨å…¬å…±å…³å…µå…¶å…·å…¸å…»å…¼å†…å†Šå†Œå†å†™å†°å†²å†³å†µå†·å‡€å‡†å‡å‡ å‡¦å‡ºå‡»å‡½åˆ†åˆ‡åˆ’åˆ—åˆ˜åˆ™åˆ›åˆåˆ åˆ¤åˆ¥åˆ©åˆªåˆ«åˆ°åˆ¶åˆ·åˆ»å‰Šå‰å‰§å‰©å‰ªå‰¯å‰²å‰µåŠ‡åŠ›åŠåŠŸåŠ åŠ¡åŠ¨åŠ©åŠ¹åŠ¿å‹•å‹™å‹¤åŒ…åŒ–åŒ—åŒ¹åŒºåŒ¿ååƒå‡åˆåå•å–å˜åšåœå å¡å°å±å³å·å¸å‚å†å‹åŸå»å‚å‰åŠå‹åŒåå‘å–å—å˜å å£å¥å¦åªå¯å°å²å³å¶å·å¸å„åˆåŒååå‘å¦å«å¬å¯å‘Šå‘˜å‘¨å‘³å‘¼å‘½å’Œå“å“ˆå“å“¡å“¦å“ªå”®å”¯å”±å•†å•Šå•å•Ÿå–‚å–„å–®å˜‰å™¨å››å›å› å›¢å›­å›³å›´å›ºå›½å›¾åœ‹åœåœ–åœŸåœ¨åœ°åœ³åœºå€å‡åå‘å—åšå‚å‹åŸåŸŸåŸ·åŸºå †å ±å ´å µå¡˜å¡å¡«å¢ƒå¢å£°å£²å¤„å¤‡å¤‰å¤å¤å¤–å¤šå¤œå¤Ÿå¤¢å¤§å¤©å¤ªå¤«å¤±å¤´å¤¹å¥‡å¥å¥–å¥—å¥³å¥¹å¥½å¦‚å¦¹å§‹å§“å§”å©‰å­å­—å­˜å­™å­¤å­¦å­¸å®å®ƒå®ˆå®‰å®Œå®˜å®šå®å®å®Ÿå®¡å®¢å®£å®¤å®¶å®¹å®½å®¾å¯„å¯†å¯Œå¯¦å¯¹å¯»å¯¼å¯¾å°å°†å°‡å°å°‘å°”å°šå°å°±å°ºå°½å°¾å±€å±‚å±å±•å±å²—å³°åµŒå·¡å·¥å·¦å·®å·±å·²å¸‚å¸ƒå¸Œå¸¦å¸§å¸®å¸°å¸³å¸¸å¹…å¹³å¹´å¹¶å¹¿åºåº“åº”åº•åºœåº¦åº«åº·å»¶å»ºå¼€å¼‚å¼ƒå¼å¼•å¼Ÿå¼ å¼±å¼µå¼ºå¼¾å½’å½“å½•å½¢å½©å½±å½¹å½¼å¾å¾„å¾…å¾ˆå¾‹å¾Œå¾—å¾ªå¾®å¾´å¿ƒå¿…å¿—å¿˜å¿œå¿ å¿«å¿µå¿½æ€æ€æ€æ€§æ€»æ¢æ¥æ¯æ‚¨æƒ…æƒ³æ„æ„Ÿæ„¿æ…‹æ…¢æˆæˆæˆ‘æˆ–æˆªæˆ³æˆ·æˆ»æ‰€æ‰‹æ‰æ‰“æ‰˜æ‰£æ‰§æ‰©æ‰«æ‰°æ‰±æ‰¹æ‰¾æ‰¿æŠ€æŠŠæŠ‘æŠ“æŠ•æŠ—æŠœæŠ¤æŠ¥æŠ½æ‹†æ‹æ‹’æ‹“æ‹”æ‹Ÿæ‹¡æ‹¥æ‹©æ‹¬æ‹¼æ‹¾æ‹¿æŒæŒ‡æŒ‰æŒ™æŸæ¢æ®æˆæ‰æ’æ˜æ¢æ¥æ§æ¨æææ’æ›æœæ­æ‘„æ’¤æ’­æ“æ“‡æ“æ“æ“´æ”¯æ”¶æ”¹æ”»æ”¾æ”¿æ•…æ•ˆæ•Œæ•—æ•™æ•°æ•´æ•¸æ–‡æ–™æ–­æ–¯æ–°æ–¹æ–¼æ—æ—‹æ—æ— æ—¢æ—¥æ—¨æ—©æ—¶æ˜‡æ˜æ˜“æ˜Ÿæ˜ æ˜¨æ˜¯æ˜µæ˜¾æ™‚æ™®æ™¯æ™ºæš‚æš—æš«æš´æ›²æ›´æ›¸æ›¼æ›¿æœ€æœƒæœˆæœ‰æœæœ›æœŸæœªæœ«æœ¬æœ¯æœºæ€æ‚æƒææŸæ¡æ¥æ°æ¿ææ„ææ—æœææ¶æŸ“æŸ¥æŸ±æŸ»æ ‡æ ˆæ æ ‘æ ¡æ ·æ ¸æ ¹æ ¼æ¡ƒæ¡†æ¡ˆæ¡œæ¡£æ¢…æ¢¦æ¢¯æ£€æ£®æ¤œæ¥­æ¥µæ¦‚æ§‹æ§½æ¨™æ¨¡æ¨£æ¨¸æ©Ÿæª”æ¬„æ¬Šæ¬ æ¬¡æ¬¢æ¬¾æ­¢æ­£æ­¤æ­¥æ®Šæ®‹æ®µæ¯æ¯æ¯æ¯æ¯”æ°”æ°´æ±‚æ±‡æ±Ÿæ± æ±ºæ²¡æ²³æ²»æ³•æ³¢æ³¨æ´»æµæµ‹æµæµæµ’æµ©æµ®æµ·æ¶ˆæ¶‰æ¶¦æ¶¨æ¶µæ·«æ·±æ·»æ¸…æ¸ˆæ¸›æ¸¡æ¸©æ¸¬æ¸¯æ¸²æ¹˜æºæº–æ»‘æ»¡æ»¤æ¼æ¼”æ¼±æ½‡ç€ç«ç°ç¶ç‚¸ç‚¹ç‚ºçƒ½ç„¡ç„¶ç…§ç†µçˆ†çˆ¬çˆ±çˆ¶ç‰‡ç‰ˆç‰›ç‰©ç‰¹çŠ¶ç‹€ç‹—ç‹›ç‹œç‹©ç‹¬çŒ›çŒ®çŒ¿ç‡ç‹ç©ç¯ç°ç­ç¾ç†ç’°ç”Ÿç”¢ç”¨ç”°ç”±ç”µç”·ç”»ç•Œç•™ç•¥ç•ªç•°ç–ç–‘ç—…ç™ºç™»ç™¼ç™½ç™¾çš„ç›Šç›‘ç›–ç›˜ç›£ç›®ç›´ç›¸çœçœ‹çœŒçœ çœ¼ç€ç¿ç¬çŸ¥çŸ©çŸ­çŸ³ç ç ”ç¡€ç¡¬ç¡®ç¢ç¢ºç¢¼ç£ç¤ºç¤¾ç¥ç¥¨ç¦ç¦»ç§ç§ç§‘ç§’ç§Ÿç§¯ç§°ç§»ç¨‹ç¨ç¨±ç©¶ç©ºçªçª—ç«‹ç« ç«¯ç¬”ç¬¦ç¬¬ç­‰ç­ç­”ç­–ç­›ç­¾ç®€ç®—ç®¡ç®±ç¯„ç¯‡ç°‡ç°¡ç°½ç±³ç±»ç²˜ç²¾ç³Šç³»ç´„ç´ç´ ç´¢ç´¯ç´°çµ‚çµ„çµçµ¡çµ¦çµ±çµµç¶™ç¶šç¶²ç·’ç·šç·´ç¸®ç¹°çº¢çº¦çº§çº¯çº²çº¹çº¿ç»ƒç»„ç»†ç»‡ç»ˆç»ç»‘ç»“ç»˜ç»™ç»œç»ç»Ÿç»§ç»©ç»­ç»´ç»¿ç¼“ç¼–ç¼©ç¼ºç½‘ç½—ç½®ç¾¤ç¾©ç¾½ç¿’ç¿»è€è€ƒè€…è€Œè€—èŠèŒè”è–èšèè¯è·è‚–è‚¡è‚²èƒ€èƒ¡èƒ½è„‘è„šè„¸è…¦è…¾è†¨è‡ªè‡³è‡´èˆ‡èˆªèˆ¬èˆ±è‰²èŠ‚èŠ±è‹¥è‹±èŒƒèè£è·èœè²èè¥è‘‰è‘—è“è—è™è™‘è™•è™Ÿè™«èè¡€è¡Œè¡è¡¨è¡°è¢«è£è£…è£œè£½è¥¿è¦è¦†è¦‹è¦è¦–è¦§è¦ªè¦½è§è§‚è§„è§†è§ˆè§’è§£è§¦è§¸è¨€è¨‚è¨ˆè¨Šè¨è¨“è¨˜è¨­è¨´è¨»è¨¼è©¢è©¦è©²è©³èªŒèªèªèª¤èª¬èª­èª°èª¿è«‹è«–è¬è­¦è­°è®€è®Šè®¡è®¢è®¤è®©è®­è®®è®¯è®°è®¸è®ºè®¾è®¿è¯è¯„è¯†è¯Šè¯è¯‘è¯•è¯è¯¢è¯¥è¯¦è¯­è¯¯è¯´è¯·è¯¸è¯»è¯¾è°ƒè°…è°±è±¡è²¯è²·è²»è²¼è³‡è³ªè´Ÿè´£è´¥è´¦è´§è´´è´¹è´¾èµ„èµ‹èµèµ–èµèµ¢èµ°èµµèµ·è¶…è¶Šè¶³è·Œè·‘è·è·Ÿè·¨è·¯è·³è¸¢èº«è¼‰è¼¸è½‰è½¦è½¬è½®è½¯è½´è½½è¾ƒè¾‘è¾“è¾è¾¨è¾¹è¾ºè¾¼è¾¾è¾¿è¿‡è¿è¿è¿‘è¿”è¿˜è¿™è¿›è¿œè¿è¿­è¿°è¿¹è¿½é€€é€é€‚é€ƒé€‰é€é€’é€”é€šé€Ÿé€ é€£é€²é€»é‡é‹ééé“é”é•é¡éµé¸é¿é‚€é‚Šé‚£é‚®é‚»éƒ¨éƒµéƒ½é„­é…é†¸é‡‡é‡Šé‡Œé‡é‡é‡‘éˆ•éˆºé‰´éŒ¯éŒ²é–é’ˆé’Ÿé’®é“¾é”€é”é”™é”®é•œé•¿é–‰é–‹é–“é–¢é–±é—œé—¨é—­é—®é—´é—»é˜€é˜…é˜ˆé˜Ÿé˜²é˜µé˜¶é™„é™…é™†é™é™é™¤é™©éšéšéš”éš›éš¨éšªéš¾é›„é›†é›¢é›£é›»éœ€é’éé é¢éŸ³é é …é †é ­é¡Œé¡é¡µé¡¹é¡ºé¡»é¢„é¢‘é¢˜é¢œé¢é¢¨é£é£“é£é£½é¥°é¦–é¦™é©—é©¬é©±éªŒé«˜é¯–é±¼é²¸é»„é»‘é»˜é»é½„ê°€ê°ê°„ê°ê°’ê°•ê°™ê°œê±°ê±´ê²ƒê²Œê²°ê²½ê³„ê³ ê³¤ê³¨ê³µê³¼ê´€ê´„ê´‘êµêµ¬êµ°ê¶Œê·€ê·œê·¸ê¸€ê¸°ê¸¸ê¹Œêº¼ê½ˆê½Œê½ê¾¸ëŒëë‚˜ë‚œë‚¨ë‚´ë‚¸ëƒë„ˆë„˜ë„¤ë…ë…¸ë…¼ëˆ„ëŠ”ëŠ˜ëŠ¥ë‹ˆë‹¤ë‹¨ë‹¬ë‹´ë‹µë‹¹ëŒ€ëŒë”ë°ë„ë…ë™ë˜ëœë¨ë©ë‘ë’¤ë“œë“ ë“¤ë“¦ë“¬ë“¯ë“±ë””ë”•ë”©ë”°ë•Œë¼ë€ë˜ë¨ëŸ¬ëŸ­ë ˆë ¤ë ¥ë ¨ë ¬ë¡€ë¡œë¡ë¡¤ë¡±ë£Œë£¨ë¥´ë¥¸ë¥¼ë¦„ë¦¬ë¦°ë¦½ë§ë§ˆë§‰ë§Œë§ë§ë§ë§¤ë§¨ë¨¸ë¨¼ë©”ë©¤ë©°ë©´ëª…ëª¨ëª©ëª»ë¬´ë¬¶ë¬¸ë¬¼ë­ë¯¸ë¯¼ë°€ë°”ë°•ë°˜ë°›ë°©ë°°ë°±ë±€ë²„ë²”ë²•ë²¤ë²¨ë³€ë³„ë³´ë³µë¶€ë¶ë¶„ë¶ˆë¹„ë¹ˆë¹Œë¹¼ì ì«ì‚¬ì‚°ì‚´ìƒìƒˆìƒ‰ìƒì„œì„ ì„¤ì„±ì„¸ì„¼ì…‹ì…”ì†Œì†ì†¡ìˆ˜ìˆœìˆ«ìŠ¤ìŠµìŠ¹ì‹œì‹ì‹ ì‹¤ì‹¬ì¨ì©ì“¸ì”©ì•„ì•ˆì•Šì•Œì•˜ì•™ì•ì•¤ì•¼ì•½ì–€ì–´ì–¸ì–»ì—†ì—ˆì—ì—”ì—˜ì—¬ì—°ì—´ì˜ˆì˜Œì˜¤ì˜¥ì˜¬ì˜´ì™€ìš”ìš©ìš°ìš´ìš¸ì›ì›¹ìœ„ìœ ìœ¨ìœ¼ì€ì„ìŒì‘ì˜ì´ì¸ì¼ì„ì…ìˆìì‘ì˜ì¡ì¥ì¬ìŸì €ì ì „ì ˆì ì •ì œì  ì ¸ì¡°ì¢…ì¢Œì£¼ì¤€ì¤‘ì¤˜ì¦ì§€ì§ˆì§±ì°Œì°¨ì°¾ì²˜ì²œì² ì²¨ì²­ì²´ì³ì´ˆì´ìµœì¶”ì¶œì¶©ì¹˜ì¹™ì¹œì¹ ì¹¨ì¹´ìº”ì»¤ì¼œì½”ì½¤ì¿ í¬í°í´í¼í‚¤í‚¬íƒ€íƒˆíƒíƒ±í„°í…Œí…í†µí‡´íˆ¬íŠ¸íŠ¹í‹€í‹°íŒ€íŒŒíŒ¨íŒ¬í¼í€íŒí˜í¬í‘œí”„í”Œí”¼í•„í•‘í•˜í•™í•œí• í•¨í•©í•«í•­í•´í–ˆí–‰í–¥í—ˆí—¬í˜„í˜•í˜¸í™”í™•í™˜íšŒí›„íœ©í‘ï¤¬ï§±ï¨Œï¨©ï¸ï¸µï»¿ï¼ï¼ˆï¼‰ï¼‹ï¼Œï¼ï¼ï¼‘ï¼šï¼›ï¼ï¼Ÿï¼©ï¼¯ï¼²ï½ï½‹ï½ï½“ï½˜ï½™ï½šï¿½ğ„‚ğŸ‡§ğŸ‡¬ğŸ‡¸ğŸ‡ºğŸˆ²ğŸƒğŸ†ğŸƒğŸ†ğŸ”ğŸ¾ğŸ‘€ğŸ‘‡ğŸ‘ŒğŸ’€ğŸ’”ğŸ’šğŸ’¥ğŸ“ğŸ“šğŸ“ğŸ“ğŸ“¡ğŸ“¦ğŸ“¨ğŸ”„ğŸ”ğŸ”ğŸ”¥ğŸ˜€ğŸ˜ğŸ˜‹ğŸ˜¢ğŸ˜°ğŸ˜µğŸš€ğŸš¨ğŸ› ğŸ¤‘ğŸ¤®ğŸ¤·ğŸ¦”\n",
      "Vocab size: 2752\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"All the unique characters:\", ''.join(chars))\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a69259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Vocabulary size: 200019\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "print(f\"Tokenizer loaded. Vocabulary size: {enc.n_vocab}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2687aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train data with 8535591 tokens.\n",
      "Loaded val data with 948400 tokens.\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[  2932, 121512,  34553,     11,   2787,    382,  11351,   2241],\n",
      "        [   352,   2616,  10289,  65762,   5874,    271,  13310,   1950],\n",
      "        [   395, 101899,    350,     49,  66991,  19853,   6294,    271],\n",
      "        [ 45212,  13575,  16879,   2402,   2697,    522,   1303,   2408]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[121512,  34553,     11,   2787,    382,  11351,   2241,     65],\n",
      "        [  2616,  10289,  65762,   5874,    271,  13310,   1950,    706],\n",
      "        [101899,    350,     49,  66991,  19853,   6294,    271,    712],\n",
      "        [ 13575,  16879,   2402,   2697,    522,   1303,   2408,  83412]])\n",
      "----\n",
      "when input is [2932] the target: 121512\n",
      "when input is [2932, 121512] the target: 34553\n",
      "when input is [2932, 121512, 34553] the target: 11\n",
      "when input is [2932, 121512, 34553, 11] the target: 2787\n",
      "when input is [2932, 121512, 34553, 11, 2787] the target: 382\n",
      "when input is [2932, 121512, 34553, 11, 2787, 382] the target: 11351\n",
      "when input is [2932, 121512, 34553, 11, 2787, 382, 11351] the target: 2241\n",
      "when input is [2932, 121512, 34553, 11, 2787, 382, 11351, 2241] the target: 65\n",
      "when input is [352] the target: 2616\n",
      "when input is [352, 2616] the target: 10289\n",
      "when input is [352, 2616, 10289] the target: 65762\n",
      "when input is [352, 2616, 10289, 65762] the target: 5874\n",
      "when input is [352, 2616, 10289, 65762, 5874] the target: 271\n",
      "when input is [352, 2616, 10289, 65762, 5874, 271] the target: 13310\n",
      "when input is [352, 2616, 10289, 65762, 5874, 271, 13310] the target: 1950\n",
      "when input is [352, 2616, 10289, 65762, 5874, 271, 13310, 1950] the target: 706\n",
      "when input is [395] the target: 101899\n",
      "when input is [395, 101899] the target: 350\n",
      "when input is [395, 101899, 350] the target: 49\n",
      "when input is [395, 101899, 350, 49] the target: 66991\n",
      "when input is [395, 101899, 350, 49, 66991] the target: 19853\n",
      "when input is [395, 101899, 350, 49, 66991, 19853] the target: 6294\n",
      "when input is [395, 101899, 350, 49, 66991, 19853, 6294] the target: 271\n",
      "when input is [395, 101899, 350, 49, 66991, 19853, 6294, 271] the target: 712\n",
      "when input is [45212] the target: 13575\n",
      "when input is [45212, 13575] the target: 16879\n",
      "when input is [45212, 13575, 16879] the target: 2402\n",
      "when input is [45212, 13575, 16879, 2402] the target: 2697\n",
      "when input is [45212, 13575, 16879, 2402, 2697] the target: 522\n",
      "when input is [45212, 13575, 16879, 2402, 2697, 522] the target: 1303\n",
      "when input is [45212, 13575, 16879, 2402, 2697, 522, 1303] the target: 2408\n",
      "when input is [45212, 13575, 16879, 2402, 2697, 522, 1303, 2408] the target: 83412\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "# -----------------------\n",
    "\n",
    "# 1. Load data from .bin files\n",
    "# We load the entire file into a numpy array\n",
    "# We MUST use the same dtype we saved it with (np.uint32)\n",
    "train_data_np = np.fromfile('/Users/samemmanuel/Documents/MS/Project Works /P-Transformer model/train.bin', dtype=np.uint32)\n",
    "val_data_np = np.fromfile('/Users/samemmanuel/Documents/MS/Project Works /P-Transformer model/val.bin', dtype=np.uint32)\n",
    "\n",
    "# 2. Convert to PyTorch tensors\n",
    "# We convert to int64 (torch.long) as this is what nn.Embedding expects\n",
    "train_data = torch.from_numpy(train_data_np.astype(np.int64))\n",
    "val_data = torch.from_numpy(val_data_np.astype(np.int64))\n",
    "\n",
    "print(f\"Loaded train data with {len(train_data)} tokens.\")\n",
    "print(f\"Loaded val data with {len(val_data)} tokens.\")\n",
    "\n",
    "# 3. The get_batch function (your code)\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # generate random starting indices for each sequence in the batch\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # stack the sequences into a batch\n",
    "    # x is the input sequence\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # y is the target sequence (shifted by one)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# 4. The test run (your code)\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "# 5. The loop to show how context builds (your code)\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a10d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2932, 121512,  34553,     11,   2787,    382,  11351,   2241],\n",
      "        [   352,   2616,  10289,  65762,   5874,    271,  13310,   1950],\n",
      "        [   395, 101899,    350,     49,  66991,  19853,   6294,    271],\n",
      "        [ 45212,  13575,  16879,   2402,   2697,    522,   1303,   2408]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd2f5abb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # token embeddings: maps each token id to a vector of logits over the vocab\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B * T, C)\n",
    "            targets_flat = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        return logits, loss\n",
    "\n",
    "m = BigramLanguageModel(model_vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print('logits.shape =', logits.shape)\n",
    "print('loss =', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3d170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P-Transformer model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
